{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNXNjybLakkW",
    "outputId": "0d208321-837e-4200-caad-89f52738650a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries ... done.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "print(\"Importing libraries ... \", end=\"\")\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6U4fqr3IWD2_",
    "outputId": "6001a5dc-1c30-4381-cba3-870696d3bb68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#run this notebook on google colab upload dataset on google drive and use from there\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2ZMIs2ufWGdG"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/datasets/brown.txt', 'r') as f: \n",
    "    data = f.read()\n",
    "data = data.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "BZrXxQQgYCUa"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(string):\n",
    "    string = string.encode('ascii', 'ignore').decode('ascii')\n",
    "    string = re.sub('^\\[.*\\]','',string)\n",
    "    string = re.sub('^.*:','',string)\n",
    "    string = string.strip()\n",
    "    string = string.strip('\\\"')\n",
    "    string = re.sub('\\?+','?',string)\n",
    "    string = re.sub('!+','!',string)\n",
    "    string = re.sub('\\.+','.',string)\n",
    "    string = re.sub(' +',' ',string)\n",
    "    string = re.sub('\\(.*\\)','',string)\n",
    "    string = string.replace(':)','')\n",
    "    string = string.replace(';)','')\n",
    "    string = string.replace('*','')\n",
    "    string = string.strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "KnlFP6xoWIrO"
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "  data[i] = data[i].replace('\\n',' ')\n",
    "  data[i] = clean(data[i])\n",
    "  data[i] = data[i] + '.'\n",
    "cleaned = []\n",
    "for i in range(len(data)):\n",
    "  if len(data[i])>=10:\n",
    "    cleaned.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu_933VwGn-n",
    "outputId": "95240297-156c-4c10-c6f8-6bdf75c2c7a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48920"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "a8tE1tjaGqa3"
   },
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "# shuffle(data)\n",
    "train_data = cleaned[:35000]\n",
    "validation_data = cleaned[35000:40000]\n",
    "test_data = cleaned[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0kqCeMra-lb",
    "outputId": "25e04f7b-527f-45bb-8b68-b0fad0f45b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Hyperparameters ... done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialising Hyperparameters ... \", end=\"\")\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "SEQUENCE_LENGTH = 2\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 64\n",
    "LR = 3e-3\n",
    "\n",
    "PADDING_IDX = 0\n",
    "LSTM_CELLS = 192\n",
    "VANILLA_CELLS = 96\n",
    "LSTM_LAYERS = 2\n",
    "LSTM_DROPOUT = 0.4\n",
    "BI_LSTM = False\n",
    "\n",
    "PREDICTION_SIZE = 20\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yle-z4PBqP1x",
    "outputId": "400e7218-2472-4e83-953e-959a9563934a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5nZVTcJLO1W",
    "outputId": "f4a97df2-959c-4353-cb89-297355af4c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn enabled: True\n"
     ]
    }
   ],
   "source": [
    "print(\"cudnn enabled:\", torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4S4AsNt7LlE_",
    "outputId": "ca5c91bd-3a9d-4b36-ce87-0ca25798a3d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name:  Tesla K80\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Name: \",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "jrc6ooPUS1Tw"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_len):\n",
    "\n",
    "        # length of sequence in consideration\n",
    "        self.seq_len = seq_len\n",
    "        # a list of all the tokens\n",
    "        self.words = self.load_words()\n",
    "        # a sorted list of all the tokens\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        # storing the vocublary size\n",
    "        self.vocab_size = len(self.uniq_words)\n",
    "\n",
    "        # dictionaries to convert to and from words\n",
    "        # used for embeddings and the generated text \n",
    "        # Note the index+1 0 is kept for padding \n",
    "        self.index_to_word = {\n",
    "            index: word for index, word in enumerate(self.uniq_words)\n",
    "        }\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(self.uniq_words)\n",
    "        }\n",
    "\n",
    "        # a grand sequence of the words, each translated \n",
    "        # into its respective indices\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "\n",
    "    def load_words(self):\n",
    "        text = ' '.join(train_data)\n",
    "        return text_to_word_sequence(text) \n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    # returns length of the dataset. required by torch\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.seq_len\n",
    "\n",
    "    # returns an item, as if accessed from the dictionary as\n",
    "    # dataset[i]. required by torch\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.seq_len]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.seq_len+1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jZlqqJE3cGVQ"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            lstm_cells,\n",
    "            lstm_layers,\n",
    "            lstm_dropout,\n",
    "            is_bidirectional,\n",
    "            vanilla_cells,\n",
    "            seq_len,\n",
    "            batch_size\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_cells = lstm_cells\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "        self.vanilla_cells = vanilla_cells\n",
    "        self.seq_len = seq_len\n",
    "        self.num_directions = 2 if is_bidirectional else 1\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=lstm_cells,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=lstm_dropout,\n",
    "            bidirectional=is_bidirectional\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            lstm_cells*self.num_directions,\n",
    "            vanilla_cells\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            vanilla_cells,\n",
    "            vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed_output = self.embedding(x)\n",
    "        lstm_output, new_state = self.lstm(embed_output, prev_state)\n",
    "        fc1_output = self.fc1(lstm_output)\n",
    "        fc2_output = self.fc2(fc1_output)\n",
    "\n",
    "        return fc2_output, new_state\n",
    "\n",
    "    def init_lstm(self):\n",
    "        return (\n",
    "            torch.zeros(\n",
    "                self.lstm_layers*self.num_directions, \n",
    "                self.seq_len,\n",
    "                self.lstm_cells\n",
    "            ),\n",
    "            torch.zeros(\n",
    "                self.lstm_layers*self.num_directions, \n",
    "                self.seq_len,\n",
    "                self.lstm_cells\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-XMZq6GmKPf",
    "outputId": "6fe868be-ddd5-4438-80b9-5e01ac467055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset ... done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Dataset ... \", end=\"\")\n",
    "dataset = Dataset(SEQUENCE_LENGTH)\n",
    "VOCAB_SIZE = dataset.vocab_size\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJ8AxD09fjBr",
    "outputId": "1bcfb89a-9f15-4bc6-9d6c-7b764b66aa28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model ... done.\n",
      "================================================================================\n",
      "MODEL:\n",
      "Model(\n",
      "  (embedding): Embedding(38835, 256)\n",
      "  (lstm): LSTM(256, 192, num_layers=2, dropout=0.4)\n",
      "  (fc1): Linear(in_features=192, out_features=96, bias=True)\n",
      "  (fc2): Linear(in_features=96, out_features=38835, bias=True)\n",
      ")\n",
      "================================================================================\n",
      "Shifting model to the GPU ... done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Model ... \", end=\"\")\n",
    "model = Model(\n",
    "    VOCAB_SIZE,\n",
    "    EMBEDDING_DIM,\n",
    "    LSTM_CELLS,\n",
    "    LSTM_LAYERS,\n",
    "    LSTM_DROPOUT,\n",
    "    BI_LSTM,\n",
    "    VANILLA_CELLS,\n",
    "    SEQUENCE_LENGTH,\n",
    "    BATCH_SIZE\n",
    ")\n",
    "print(\"done.\")\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL:\")\n",
    "print(model)\n",
    "print(\"=\"*80)\n",
    "print(\"Shifting model to the GPU ... \", end=\"\")\n",
    "model.to(DEVICE)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5h5U4o64-XZ",
    "outputId": "3c166de9-2a6e-435a-d3b4-a2e05ffaa312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up loss and optimizers ... done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up loss and optimizers ... \", end=\"\")\n",
    "\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = optim.Adam(model.parameters(), lr=LR)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ALNAE-jtp2Fr"
   },
   "outputs": [],
   "source": [
    "def train(dataset, model):\n",
    "    # getting the model ready for training ...\n",
    "    model.train()\n",
    "    # preparing the data to be iterated over\n",
    "    data_generator = DataLoader(dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "    \n",
    "    # stores losses for all the epochs\n",
    "    loss_values = []\n",
    "    # stores a list of the epoch number\n",
    "    epoch_numbers = []\n",
    "    \n",
    "    # going over all the epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        # initialising the model for each epoch\n",
    "        h_state, c_state = model.init_lstm()\n",
    "        \n",
    "        # sending the inits to the GPU\n",
    "        h_state, c_state = h_state.to(DEVICE), c_state.to(DEVICE)\n",
    "\n",
    "        # iterating over the dataset\n",
    "        for batch, (x, y) in enumerate(tqdm(data_generator)):\n",
    "            # transferring data to the GPU\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            # resetting the gradients for each batch \n",
    "            # for proper training\n",
    "            OPTIMIZER.zero_grad()\n",
    "\n",
    "            # predictions from the model\n",
    "            y_pred, (h_state, c_state) = model(x, (h_state, c_state))\n",
    "\n",
    "            loss = CRITERION(y_pred.transpose(1, 2), y)\n",
    "            h_state = h_state.detach()\n",
    "            c_state = c_state.detach()\n",
    "            loss.backward()\n",
    "            OPTIMIZER.step()\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "        epoch_numbers.append(epoch+1)\n",
    "        print(f\"Epoch: {epoch}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZRUPHvwYrER",
    "outputId": "92bbf7bf-1a81-4b88-baf9-2022473fcc00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12030/12030 [07:49<00:00, 25.60it/s]\n",
      "  0%|          | 4/12030 [00:00<06:20, 31.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 5.82609224319458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12030/12030 [07:49<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 5.690529823303223\n"
     ]
    }
   ],
   "source": [
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhyzTp8IZLOa",
    "outputId": "b1b03a38-8d12-4dac-e972-5b2ac398db18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([296.0504])\n"
     ]
    }
   ],
   "source": [
    "perplexity  = torch.exp(torch.Tensor([5.690529823303223]))\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "IOOp-UNsMPu8"
   },
   "outputs": [],
   "source": [
    "x = datetime.datetime.now()\n",
    "current_time = str(x.year)+\"-\"+str(x.month)+\"-\"+str(x.day)+\"-\"+str(x.hour)+\"-\"+str(x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mEYDu4iLFGr-"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{current_time}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "h9R3pYSFM_Ma"
   },
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    \"EMBEDDING_DIM\": EMBEDDING_DIM,\n",
    "    \n",
    "    \"SEQUENCE_LENGTH\": SEQUENCE_LENGTH,\n",
    "\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LR\": LR,\n",
    "\n",
    "    \"LSTM_CELLS\": LSTM_CELLS,\n",
    "    \"VANILLA_CELLS\": VANILLA_CELLS,\n",
    "    \"LSTM_LAYERS\": LSTM_LAYERS,\n",
    "    \"LSTM_DROPOUT\": LSTM_DROPOUT,\n",
    "    \"BI_LSTM\": BI_LSTM,\n",
    "\n",
    "    \"PREDICTION_SIZE\": PREDICTION_SIZE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "kvEAOj8fN3EO"
   },
   "outputs": [],
   "source": [
    "with open(f'{current_time}.json', 'w+') as fp:\n",
    "    json.dump(HYPERPARAMETERS, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnEFDpKvvSrM",
    "outputId": "407bd9f5-b75a-4eef-844e-c20f34ae7ac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(38835, 256)\n",
       "  (lstm): LSTM(256, 192, num_layers=2, dropout=0.4)\n",
       "  (fc1): Linear(in_features=192, out_features=96, bias=True)\n",
       "  (fc2): Linear(in_features=96, out_features=38835, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shifting model back to the CPU\n",
    "model.to('cpu')\n",
    "# getting the model ready for testing ...\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "qWUIDzcGq_U8"
   },
   "outputs": [],
   "source": [
    "def generate(dataset, model, seed):\n",
    "    # # shifting model back to the CPU\n",
    "    # model.to('cpu')\n",
    "    # # getting the model ready for testing ...\n",
    "    # model.eval()\n",
    "    # a list of words\n",
    "    words = text_to_word_sequence(seed)\n",
    "    # # initialising the model parameters    \n",
    "    h_state, c_state = model.init_lstm()\n",
    "    # going over, and predicting\n",
    "    x = torch.tensor([[dataset.word_to_index[w] for w in words[-2:]]])\n",
    "    \n",
    "    y_pred, (h_state, c_state) = model(x, (h_state, c_state))\n",
    "    loss = CRITERION(y_pred.transpose(1, 2), x)\n",
    "    perplexity  = torch.exp(torch.Tensor([loss.item()]))\n",
    "    perp = int(perplexity[0])\n",
    "    #print(perp)\n",
    "    return perp/len(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kCQTuRpVjaL",
    "outputId": "28580ced-a4e1-45b4-ae52-2430e26e3a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "seeds = validation_data\n",
    "lm = []\n",
    "lines = []\n",
    "tot, num = 0, 0\n",
    "for i in range(len(seeds)):\n",
    "    # seed_list = seeds[i].split()\n",
    "    # seed = \"\"\n",
    "    # for word in seed_list:\n",
    "    #     seed += word +\" \"\n",
    "    # seed = seed[:-1]\n",
    "    if i%3000==0:\n",
    "      print(i)\n",
    "    try:\n",
    "      perplexity = generate(dataset, model, seed=seeds[i])\n",
    "      tot += perplexity\n",
    "      num += 1\n",
    "      sent = f'{seeds[i]}\\t{perplexity}'\n",
    "      lines.append(sent)\n",
    "      \n",
    "      \n",
    "    except:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAv_OQhZu7Tg",
    "outputId": "d4650f4e-4d20-487c-a949-2988590d15e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Greatest thing that ever happened.\\t419.0',\n",
       " \"Now Eileen really would have to settle down to love honor and obey, and she'd have to quit drinking.\\t1828.6315789473683\",\n",
       " \"He'd come East for the christening, by God he would.\\t41.2\",\n",
       " 'Before he left town Pat saw to it that I was fixed up with a job.\\t2195.9375',\n",
       " 'Pat had contacts all over the labor movement.\\t512.75']"
      ]
     },
     "execution_count": 164,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "gLhUWuLAltV2"
   },
   "outputs": [],
   "source": [
    "with open('validation_lstm.txt', 'w') as f:\n",
    "    for item in lines:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSA36aYOrWxc",
    "outputId": "39b4fd51-d1b7-4da5-ae3f-0f5dbef96097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21781.48651231239"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "FbUQrweksMsh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nlp assignment 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
